%!TEX ROOT=ctutest.tex

\chapter{Theory}

In the previous chapter, we have briefly introduced MDPs. In this chapter, we will build on that intuition by reviewing a small part of the theory behind them, and we will also show possible approaches to their solving.

\section{Environment}
The environment that we introduced so far contains states, actions, transition probabilities, and costs for each action. However, we did not assign any rules to it! 
Without these rules, one can not create any connection between states and actions or actions and transition probabilities.

The environmental rules could range anywhere from the ones applied in the crossword puzzle game (fully observable, deterministic, sequential, static, discrete with single-agent) up to the ones applied in the taxi driving planning (partially observable, stochastic, sequential, dynamic, continuous with multiple agents). We will not describe the meaning of all specific rules in this paper, as it is not the project's task. In case the reader is interested, we refer to read  \cite{russel2010} PRIDAT STRANKY DO REFERENCE asi 45-48.

\newpage


For the sake of simplicity, let us assume the following properties of the given environment:
\begin{description}
  \item[$\bullet$ Fully observable] The agent has all information needed about the environment.
  \item[$\bullet$ Stochastic] The movement of the agent is not certain. The result of movement can be different from the desired one as a result of noise,
  \item[$\bullet$ Sequential] The current action is going to have an impact on future actions,
  \item[$\bullet$ Static] The state of the environment can not change,
  \item[$\bullet$ Discrete] The environment has a finite number of states and actions.
\end{description}



\section{MDP and its features}
With environmental properties sorted out, let us move on to MDPs themselves. \\
Following theory are taken over from \cite{Kolobov2012}.
We define MDPs, as in \cite{Kolobov2012}, slightly modified for our purpose.
  \\ \\

\begin{definition}[\textbf{Fully Observable Markov Decision Process (MDP)}]
A fully observable MDP is a tuple (S, A, D, T, R), where:
\begin{description}
  \item[$\bullet$ ] $S$ is the finite set of all possible states of the system, also called the state space;
  \item[$\bullet$ ] $A$ is the finite set of all actions an agent can take;
  \item[$\bullet$ ] $D$ is a finite or infinite sequence of the natural numbers of the form $(1, 2, 3, \ldots, T_{max})$ or $(1, 2, 3, \ldots)$ respectively, 
        denoting the decision epochs, also called time steps, at which agent needs to act;
  \item[$\bullet$ ] $T : S \times A \times S \rightarrow [ \,0, 1] \,$ is a transition function, a mapping specifying the probability $T(s_1, a, s_2)$ of going to state $s_2$ if action $a$ is executed when the agent is in state $s_1$ at time step $t$;
  \item[$\bullet$ ] $R : S \times A \times S \rightarrow R$ is a reward function that gives a finite numeric reward value $R(s_1, a, s_2)$ obtained when the system goes from state $s_1$ to state $s_2$ as a result of executing action $a$ at time step $t$.
\end{description}

\end{definition}

Every MDP is solving its problem according to its movement policies. As the number of policies itself can be high order polynomial, we are going to relax their definition as well.
\\ \\
\begin{definition}[\textbf{Markovian Policy}]
A probabilistic (deterministic) history-dependent policy $\pi: H \times A \rightarrow [ \,0, 1] \,(\pi: H \rightarrow A)$ is \textit{Markovian} if for any two histories $h_{s,t}$ and $h'_{s,t}$, both of which end in the same state $s$ at the same time step $t$, and for any action $a$, 
$\pi(h_{s,t}, a) = \pi(h'_{s,t}, a)$ $(\pi(h_{s,t}) = a$ if and only if $\pi(h_{s,t}) = a)$.
\end{definition}

Every state in every time step is then evaluated according to \textbf{value function}. A Markovian value function then corresponds to $ V: S \times D 
\rightarrow [ \,-\infty, \infty] \,$. In the following text we will refer to value function as $V(s, t)$ or $V(s)$.

The value function of a policy is then: \\ \\

\begin{definition}[\textbf{The Value Function of a Policy}]
Let $h_{s,t}$ be a history that terminates at state $s$ and time $t$. Let $R^{\pi_{h_{s,t}}}_{t'}$ be random variables for the amount of reward obtained in an MDP as a result of executing policy $\pi$ starting in state $s$ for all time steps $t'$ s.t. $t \leqslant t' \leqslant |D|$ if the MDP ended up in state $s$ at time $t$ via history $h_{s,t}$.The value function $V^{\pi}: H \rightarrow [ \,−\infty,\infty] \,$ \textit{of a history-dependent policy} $\pi$ is a utility function $u$ of the reward sequence $R^{\pi_{h_{s,t}}}_t, R^{\pi_{h_{s,t}}}_{t+1}, \ldots$ that one can accumulate by executing $\pi$ at time steps $t, t + 1,\ldots$  after history $h_{s,t}$. Mathematically, $V^{\pi} (h_{s,t}) = u(R^{\pi_{h_{s,t}}}_t , R^{\pi_{h_{s,t}}}_{t+1}, \ldots)$.
\end{definition}

Among the policies evaluated according to the previous rule, we will be able to find an \textbf{optimal MDP solution} denoted as $\pi^*$ with value $V^*$ called the optimal value function. Such optimal solution then satisfies $V^{*} (h) \geqslant V^{\pi} (h)$ for all histories $h \in H$.

One of the possible approaches to evaluate the value function is using Expected Linear Additive Utility.
\\
\begin{definition}[\textbf{Expected Linear Additive Utility}]
An \textit{expected linear additive utility} function is a function $u(R_t, R_{t+1}, \ldots) = E[ \,\sum_{t'=t}^{|D|} \gamma ^{t'−t} R_{t'}] \, = E[ \,\sum_{t'=0}^{|D|-t} \gamma^{t'} R_{t'+t}] \,$ that computes the utility of a
reward sequence as the expected sum of (possibly discounted) rewards in this sequence, where $gamma \geqslant 0$
is the discount factor.
\end{definition}

This approach eliminates the possibility of multiple different solution values resulting from multiple stochastic runs of the same policy. It employs the expected value and does not need to perform vectors equality comparison as its results are scalars.

It turns out that this approach is even better as it guarantees a fundamental property of MDPs called \textbf{The Optimality Principle}, according to which \textit{among the policies that are evaluated by the expected linear additive utility, there exists a policy that is optimal at every time step.}

\section{MDPs techniques}

This part will briefly introduce a few fundamental algorithms for MDPs solving: Brute-Force algorithm, Policy Iteration, Value Iteration, and show a possible solution for its disadvantages: Prioritizations. \\
After reading this section, the reader should know the possibilities of solving MDP and be ready for the following section, where we will discuss the finite horizon's advantages.

\subsubsection{Brute-Force Algorithm}
Brute-force algorithm, as its name suggests, is the one most naive algorithm, that is similar for all classes of problem solving. In the proccess of evaluating the problem, the method evaluates all possible combinations and chooses the best one.
And it is obviously not used in vast majority of cases for this reason as the Algorithm needs to evaluate $|S|^{A}$ policies.
$S$ in the previous equation represents states and $A$ actions.

However, as the number of states, or actions, rises, the number of such evaluation becomes huge. Another problem comes from the fact, that actions in the environment are not deterministic and often cyclic. Both of these problems results in steep complexity rise.

That is why another approach comes in.

\subsubsection{An iterative approach to policy evaluation}

The evaluation of cyclic environments requires suitable equation which captures all movements, transition and rewards in it.
That means: value function of every state should correspond to sum of rewards for moving towards successor states. Moreover, the agent should maximize its reward by getting to goal, so the value function of the successor state multiplied by its probability should also appear in equation. This formula can be written as:\\
\begin{equation}
\begin{aligned}
V^{\pi} (s) & = 0 && \text{(if $s \in G$)} \\
& = sum_{s' \in S} T(s, \pi (s), s') [ \,C(s, \pi(s), s') + V^{\pi} (s')] \, && \text{(otherwise)}
\end{aligned}
\end{equation}

This system of linear equation reduced the complexity of previous approach. It \textit{can} be solved using Gaussian Elimination in $O(|S|^3)$. It is still not enough efficient, but its idea can be used to solve it with \textbf{an iterative approach}.

The iterative approach does exactly what it says. 
It starts with rough estimations and continuously works its way up to the same solution as of the linear system. In addition, this solution is also optimal \cite{Kolobov2012}. As It stores previous iteration it can be described as a part of dynamic programming. Every iteration of this algorithm runs in $O(|S|^2)$.

\LinesNumbered
\begin{algorithm}[H]
\SetAlgoLined
//Assumption: $\pi$ is proper (ends up in a goal state) \\
initialize $V^{\pi}_0$ arbitrarily for each state \\
$n \xleftarrow{} 0$ \\
\Repeat{$ max_{s\in S} residual_n (s) \leq \epsilon$}{
$n \xleftarrow{} n + 1$ \\
\ForEach{$ s \in S $}{
compute $V^{\pi}_n (s) \xleftarrow{} \sum_{s' \in S} T (s, \pi (s), s') [ \,C(s, \pi (s), s') + V^{\pi}_{n−1} (s') ] \,$ \\
compute $residual_n (s) \xleftarrow{} |V^{\pi}_n (s) − V^{\pi}_{n−1} (s)| $ \\
}
}
return $V^{\pi}_n$
\caption{Iterative Policy Evaluation}
\end{algorithm}

Now we know how to evaluate the states of the environment given some policy, but we did not yet described how to choose the best policy for given evaluation. 

\subsubsection{Policy iteration}
Given the fact that result of iterative policy evaluation is optimal after undefined number of iterations for specific policy, we can further improve it by iterating between policy evaluation and policy improvement.
For every iteration, we have \textif{almost} optimal policy evaluation for previous policy and with that we can improve the policy by iterating over possible actions from all states and choose that action, if is has better results, than previous one. The algorithm stops when the policy remains without a change at the end of following iteration.

\LinesNumbered
\begin{algorithm}[H]
\SetAlgoLined
initialize $\pi_0$ to be an arbitrary proper policy \\
$n \xleftarrow{} 0$ \\
\Repeat{$\pi_n == \pi_{n−1}$ }{
    $n \xleftarrow{} n + 1$ \\
    Policy Evaluation: compute $V^{\pi_{n−1}}$ \\
    Policy Improvement: \\
    \ForEach{state $s \in S$}{
        $ \pi_n (s) \xleftarrow{} \pi_{n−1} (s) $ \\
        $ \forall a \in A$ compute $Q^{(V^{\pi_{n−1}})} (s, a) $ \\
        $ V_n (s) \xleftarrow{} min_{a \in A} Q^{(V^{\pi_{n−1}})} (s, a) $ \\
        \uIf{$ Q^{(V^{\pi_{n−1}})} (s, \pi_{n−1} (s)) > V_n (s)$}{
            $ \pi_n (s) \xleftarrow{} argmin_{a \in A} Q^{(V^{\pi_{n−1}})} (s, a)$
        }
    \textbf{end}
    }
}
return $\pi_n$
\caption{Policy Iteration}
\end{algorithm}


\\
\\
\\
\\
\\
\\
\\
\\
\\
\\


\LinesNumbered
\begin{algorithm}[H]
\SetAlgoLined
initialize $V_0$ arbitrarily for each state \\
$n \xleftarrow{} 0$ \\ 
\Repeat{$max_{s \in S} residual_n (s) \let \epsilon$}{$n \xleftarrow{} n + 1$ \\\ForEach{$s \in S$}{compute $V_n (s)$ using Bellman backup at $s$\\ compute $residual_n (s) = |V_n (s) - V_{n-1} (s)|$}}
return greedy policy: $\pi^{V_n} (s) = argmin_{a\inA} \sum_{s'\inS} \tau(s, a, s')[ \,C(s, a, s') + V_n (s')] \,$
\caption{Value Iteration}
\end{algorithm}


\LinesNumbered
\begin{algorithm}[H]
\SetAlgoLined
initialize $V_0$ arbitrarily for each state \\
$n \xleftarrow{} 0$ \\
\Repeat{$ max_{s \in S} residual_n (s) \let \epsilon $}{}
return greedy policy: $\pi^{V_n} (s) = argmin_{a\inA} \sum_{s'\inS} \tau(s, a, s')[ \,C(s, a, s') + V_n (s')] \,$
\caption{Value Iteration}
\end{algorithm}



