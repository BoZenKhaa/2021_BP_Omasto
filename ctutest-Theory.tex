%!TEX ROOT=ctutest.tex

\chapter{Theory}

In the previous chapter, we have briefly introduced MDPs. In this chapter, we will build on that intuition by reviewing a small part of the theory behind them, and we will also show possible approaches to their solving.

\section{Environment}
The environment that we introduced so far contains states, actions, transition probabilities, and costs for each action. However, we did not assign any rules to it! 
Without these rules, one can not create any connection between states and actions or actions and transition probabilities.

The environmental rules could range anywhere from the ones applied in the crossword puzzle game (fully observable, deterministic, sequential, static, discrete with single-agent) up to the ones applied in the taxi driving planning (partially observable, stochastic, sequential, dynamic, continuous with multiple agents). We will not describe the meaning of all specific rules in this paper, as it is not the project's task. In case the reader is interested, we refer to read  \cite{russel2010} PRIDAT STRANKY DO REFERENCE asi 45-48.

\newpage


For the sake of simplicity, let us assume the following properties of the given environment:
\begin{description}
  \item[$\bullet$ Fully observable] The agent has all information needed about the environment.
  \item[$\bullet$ Stochastic] The movement of the agent is not certain. The result of movement can be different from the desired one as a result of noise,
  \item[$\bullet$ Sequential] The current action is going to have an impact on future actions,
  \item[$\bullet$ Static] The state of the environment can not change,
  \item[$\bullet$ Discrete] The environment has a finite number of states and actions.
\end{description}



\section{MDP and its features}
With environmental properties sorted out, let us move on to MDPs themselves. \\
Following theory are taken over from \cite{Kolobov2012}.
We define MDPs, as in \cite{Kolobov2012}, slightly modified for our purpose.
  \\ \\

\begin{definition}[\textbf{Fully Observable Markov Decision Process (MDP)}]
A fully observable MDP is a tuple (S, A, D, T, R), where:
\begin{description}
  \item[$\bullet$ ] $S$ is the finite set of all possible states of the system, also called the state space;
  \item[$\bullet$ ] $A$ is the finite set of all actions an agent can take;
  \item[$\bullet$ ] $D$ is a finite or infinite sequence of the natural numbers of the form $(1, 2, 3, \ldots, T_{max})$ or $(1, 2, 3, \ldots)$ respectively, 
        denoting the decision epochs, also called time steps, at which agent needs to act;
  \item[$\bullet$ ] $T : S \times A \times S \rightarrow [ \,0, 1] \,$ is a transition function, a mapping specifying the probability $T(s_1, a, s_2)$ of going to state $s_2$ if action $a$ is executed when the agent is in state $s_1$ at time step $t$;
  \item[$\bullet$ ] $R : S \times A \times S \rightarrow R$ is a reward function that gives a finite numeric reward value $R(s_1, a, s_2)$ obtained when the system goes from state $s_1$ to state $s_2$ as a result of executing action $a$ at time step $t$.
\end{description}

\end{definition}

Every MDP is solving its problem according to its movement policies. As the number of policies itself can be exponential, we are going to relax their definition as well.
\\ \\
\begin{definition}[\textbf{Markovian Policy}]
A probabilistic (deterministic) history-dependent policy $\pi: H \times A \rightarrow [ \,0, 1] \,(\pi: H \rightarrow A)$ is \textit{Markovian} if for any two histories $h_{s,t}$ and $h'_{s,t}$, both of which end in the same state $s$ at the same time step $t$, and for any action $a$, 
$\pi(h_{s,t}, a) = \pi(h'_{s,t}, a)$ $(\pi(h_{s,t}) = a$ if and only if $\pi(h_{s,t}) = a)$.
\end{definition}

Every state in every time step is then evaluated according to \textbf{value function}. A Markovian value function then corresponds to $ V: S \times D 
\rightarrow [ \,-\infty, \infty] \,$. In the following text we will refer to value function as $V(s, t)$ or $V(s)$.

The value function of a policy is then: \\ \\

\begin{definition}[\textbf{The Value Function of a Policy}]
Let $h_{s,t}$ be a history that terminates at state $s$ and time $t$. Let $R^{\pi_{h_{s,t}}}_{t'}$ be random variables for the amount of reward obtained in an MDP as a result of executing policy $\pi$ starting in state $s$ for all time steps $t'$ s.t. $t \leqslant t' \leqslant |D|$ if the MDP ended up in state $s$ at time $t$ via history $h_{s,t}$.The value function $V^{\pi}: H \rightarrow [ \,−\infty,\infty] \,$ \textit{of a history-dependent policy} $\pi$ is a utility function $u$ of the reward sequence $R^{\pi_{h_{s,t}}}_t, R^{\pi_{h_{s,t}}}_{t+1}, \ldots$ that one can accumulate by executing $\pi$ at time steps $t, t + 1,\ldots$  after history $h_{s,t}$. Mathematically, $V^{\pi} (h_{s,t}) = u(R^{\pi_{h_{s,t}}}_t , R^{\pi_{h_{s,t}}}_{t+1}, \ldots)$.
\end{definition}

Among the policies evaluated according to the previous rule, we will be able to find an \textbf{optimal MDP solution} denoted as $\pi^*$ with value $V^*$ called the optimal value function. Such optimal solution then satisfies $V^{*} (h) \geqslant V^{\pi} (h)$ for all histories $h \in H$.

One of the possible approaches to evaluate the value function is using Expected Linear Additive Utility.
\\
\begin{definition}[\textbf{Expected Linear Additive Utility}]
An \textit{expected linear additive utility} function is a function $u(R_t, R_{t+1}, \ldots) = E[ \,\sum_{t'=t}^{|D|} \gamma ^{t'−t} R_{t'}] \, = E[ \,\sum_{t'=0}^{|D|-t} \gamma^{t'} R_{t'+t}] \,$ that computes the utility of a
reward sequence as the expected sum of (possibly discounted) rewards in this sequence, where $gamma \geqslant 0$
is the discount factor.
\end{definition}

This approach eliminates the possibility of multiple different solution values resulting from multiple stochastic runs of the same policy. It employs the expected value and does not need to perform vectors equality comparison as its results are scalars.

It turns out that this approach is even better as it guarantees a fundamental property of MDPs called \textbf{The Optimality Principle}, according to which \textit{among the policies that are evaluated by the expected linear additive utility, there exists a policy that is optimal at every time step.}

\section{MDPs techniques}

This part will briefly introduce a few fundamental algorithms for MDPs solving: Brute-Force algorithm, Policy Iteration, Value Iteration, and show a possible solution for its disadvantages: Prioritizations. \\
After reading this section, the reader should know the possibilities of solving MDP and be ready for the following section, where we will discuss the finite horizon's advantages.



\LinesNumbered
\begin{algorithm}[H]
\SetAlgoLined
initialize $V_0$ arbitrarily for each state \\
$n \xleftarrow{} 0$ \\
\Repeat{$max_{s \in S} residual_n (s) \let \epsilon$}{
    $n \xleftarrow{} n + 1$ \\
    \ForEach{$s \in S$}{
        compute $V_n (s)$ using Bellman backup at $s$\\
        compute $residual_n (s) = |V_n (s) - V_{n-1} (s)|$
    }
}
return greedy policy: $\pi^{V_n} (s) = argmin_{a \in A} \sum_{s' \in S} \tau(s, a, s')[ \,C(s, a, s') + V_n (s')] \,$
\caption{Value Iteration}
\end{algorithm}