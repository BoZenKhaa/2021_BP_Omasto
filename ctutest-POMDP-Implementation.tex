
\chapter{Implementation}

This chapter is focused on the practical part of the thesis. IN the beginning, we present the assigned tasks connected with this paper. Then, we follow up with the analysis and description of the implemented methods. The sections after that present the benchmark problems and in the end, the validations and benchmarks are presented.

Assignment

The task of this paper is to survey and extend the methods implemented in the JuliaPOMDP library, with the emphasis on the finite horizon ones. To add any further methods or interfaces missing to implement such methods and to maintain the interoperability of the library. To evaluate the performance of the implemented methods and design new test instances, if applicable. And, if applicable, to benchmark the methods against other methods implemented in the library.


Design of the JuliaPOMDP library

Before we dive into the description of the tasks accomplished. It is necessary to introduce the JuliaPOMDP library itself.
The library is based on a set of interfaces gradually built on each other. Such foundations allow the community to create an environment in which the users can almost seamlessly change the functionality of their code. The interface offers uniform methods for defining problems (such as states for getting all states of the problem), structures for storing results in the policies (AlphaVectorPolicy for storing $\alpha$-vectors of POMDPs), passing the parameters to the solver(, or for executing the algorithm selected (calling `solve` is the same no matter the problem or the solver).

Methods defined in JuliaPOMDP

JuliaPOMDP's base method is the interface called POMDPs.jl. This interface defines all methods ranging from defining the model, through storing the result, to defining and executing solvers. On top of these methods, the user can either define a new problem or use already defined one in POMDPModels.jl repository. The user can also define a new solver, or use already defined one. Furthermore, the solvers return their results wrapped into a policy dedicated for it in POMDPPolicies.jl. If implementing a solver, the user would probably use the tools defined in POMDPModelTools.jl as, for example, an `ordered\_vector` which returns the wanted vector in an ordered manner, or distributions defined in the same repository.

ZMINIT ZE JE TO DIKY TOU INTEROPERABLE

The library also contains a lot of solvers for both MDPs and POMDPs. Ranging from Value Iteration up to Monte Carlo Tree Search solver for MDPs (4 total) and from approximate QMDP and other approximate solvers up to SARSOP or Incremental Prunning for POMDPs (12 total). Some of these solvers are reaching optimal solutions and some, usually approximate solvers, only suboptimal results, meaning that some are used more than the others. And they are also evaluated accordingly. However, with toal of 18 solvers (with 2 more reinforcement solvers) are offering more than enough to choose from when solving Infinite Horizon (PO)MDPs. 

Which concludes our analysis of the library.

The Finite Horizon POMDP solvers that are the main task of this thesis, have not yet been included in the library at the time of the assignment. Moreover, the interface deemed essential for such solvers has been proposed, but not completed. Such an interface is needed as the solvers need to operate over single stages while the current interface does not support this type of handling. Because of that, the objective of this thesis became to implement the selected algorithms as the proof of work. For that, the finite horizon value iteration was selected for MDPs and point-based finite horizon value iteration for POMDPs. These algorithms are the foundations for many improvements already published, making them an excellent starting point for future additions to the library. These algorithms are introduced in [TODO: DOOPLNIT CITACE NA PAPERY] and are recommended implementations to any reader interested in developing a new algorithm for JuliaPOMDP library. 

The algorithms were to be evaluated against their infinite horizon counterparts. While implementing them, we found out that the infinite horizon point based value iteration algorithm implemented in library was formerly implemented for specific POMDPs with only two states. Thus, the task of this thesis was extended to fix and update of infinite horizon point based value iteration.




FINITE HORIZON POMDPs

This chapter introduces the interface for defining Finite Horizon POMDPs. The interface was implemented in line with the initial design proposed by Zachary Sunberg in May 2019. The final version of this interface was implemented by both the author and Zachary Sunberg, who implemented the foundations of this interface in such a way that it is built on top of other interfaces, thus remaining interoperable, and preserved the idea of library. For his part in designing this interface that opens a whole new branch of POMDP algorithms and his help during this year, we are endlessly grateful.




The goal of the Finite Horizon POMDPs interface was to create a POMDPs.jl-compatible interface for defining MDPs and POMDPs with finite horizons. 
Its key ideas were in particular, to :
\begin{itemize}
    \item provide a way for value-iteration-based algorithms to start at the final stage and work backward
    \item be compatible with generic POMDPs.jl solvers and simulators
    \item provide a Finite-Horizon wrapper for Infinite-Horizon MDPs.
    \item be compatible with other interface extensions like constrained POMDPs and mixed observability problems.
\end{itemize}

# MOZNA TO NEJAK OKECAT

And its implementation was to contain the declaration of the following interface functions:

\begin{itemize}
    \item \textit{HorizonLength(::Type{<:Union{MDP,POMDP}}) = InfiniteHorizon()}
    \begin{itemize}
        \item \textit{FiniteHorizon}
        \item \textit{InfiniteHorizon}
    \end{itemize}
    \item \textit{horizon(m::Union{MDP,POMDP})::Int}
    \item \textit{stage\_states(m::Union{MDP,POMDP}, t::Int)}
    \item \textit{stage\_stateindex(m::Union{MDP,POMDP}, t::Int, s)}
    \item \textit{stage\_actions(m::Union{MDP,POMDP}, t::Int, [s])}
\end{itemize}

The whole idea of the interface lies on the concept of the HorizonLength type, which returns the FiniteHorizon of InfiniteHorizon type corresponding to either being finite or infinite horizon (PO)MDP. In case where (PO)MDP is a finite horizon, the horizon value can be obtained by calling the `horizon` method. Furthermore, the interface extends the methods 
defined for infinite horizon problems by declaring their finite horizon counterparts. Methods belonging to this group are: $stage\_states$  which returns states for given stage only, $stage\_stateindex$ which returns the index of a given state in a given stage, $stage\_observations$ which returns observations for a given stage and \[TODO: DOPLNIT STAGE_OBSINDEX DO INTERFACE.JL\] $stage\_obsindex$ which returns the index of a given observation in a given stage

By using the combination of methods from FiniteHorizonPOMDPs.jl and other interfaces, the user is able to define his own finite horizon (PO)MDP. At the same time, we recognize it can be difficult, or even lengthy to define one. For exactly these cases, the interface concept contained the proposal of an utility, whose goal is to transform the user-defined infinite horizon (PO)MDP into a finite horizon one.  The design concept of the proposal was formerly to define a simple function that would transform a received infinite horizon (PO)MDP into a finite horizon (PO)MDP wrapper:

\begin{itemize}
    \item \textit{fixhorizon(m::Union{MDP,POMDP}, T::Int) creates one of}
    \begin{itemize}
        \item \textit{FiniteHorizonMDP{S, A} <: MDP{Tuple{S,Int}, A}}
        \item \textit{FiniteHorizonPOMDP{S, A, O} <: POMDP{Tuple{S,Int}, A, O}}
    \end{itemize}
\end{itemize}

Finally, this utility became a set of methods that could easily create a repository of its own. However, it is necessary to keep the utility with its original interface to be easily available. 

Other than $fixhorizon$ method, the utility defines new output types to recognize transformed (PO)MDPs. These methods are called FixedHorizonMDPWrapper and FixedHorizonPOMDPWrapper, if there is no need to differ between them, the FHWrapper type is used. Thanks to this distinction, Julia Programming language is able to dynamically dispatch the correct method on each call.

To distinct between the states of the infinite horizon (PO)MDP and the finite horizon one, the $fixhorizon$ utility has introduced a new structure for holding staged states and observations. This was done by wrapping the pairs of states or observations and stages into tuples, encoding the stage information without much performance lost.

Having defined the type of (PO)MDP and its states and observations, the utility also defines other interface methods required by various solvers. Namely, the methods from POMDPs.jl interface ensure proper functionality no matter the finality of (PO)MDP as $transition$, $reward$, $isterminal$, $actions$, $discount$ etc. Methods defined specifically for finite horizon (PO)MDPs from POMDPs.jl interface which generally return all states or observations for all stages. For example, $states$, $stateindex$, $observations$ or $obsindex$. And finite horizon (PO)MDPs interface methods implemented in this interface and described in the text above, such as $stage\_states$ etc. Furthermore, the $fixhorizon$ utility implements other methods from POMDPModelTools, which defines distributions for (PO)MDPS or methods for ordering states, actions, or observations. Or from Random, offering the mean to compute probabilistic data about the distributions - mean, mode, support or pdf.

# TOHLE TADY MOZNA NEMUSI BYT< JE POTREBA ZKONTROLOAT JESTLI SJEM TO NEREKOL UZ V TEXTU NAHORE A JAK MOC SE TO PREKRYVA

To define the MDP instance, we have to define the methods declared in \textit{JuliaPOMDP/POMDPs.jl} \cite{JMLR:v18:16-300} or from another library's package. The Finite-Horizon MDP Solver further requires: \textit{stage\_states}, which returns states of a given epoch, \textit{stage\_actions}, which return actions of a given epoch, and \textit{stage\_stateindex}, which return indices of a given state for a given epoch. For example of properly defined MDP see directory \textit{/test/instances}.

TODO: ZMINIT KDE SE NACHAZEJI IMPLEMENTACE





Finite Horizon Value Iteration




\section{Implementation}

The implementation is at \cite{FHPOMDP} or APENDIX.

The Finite Horizon algorithm evaluates a given MDP problem using Value Iteration.  





POPSAT JEDNOTLIVE METODY INTERFACE a ZMINIT I TO, ZE JSME NEKTERE PRIDALI

\subsection{Solution approach}

 The Current solution consists of a solver \textit{mysolve(mdp)} iterating and evaluating epochs and \textit{FiniteHorizonPolicy} structure storing its results (value function matrix, matrix of Qs, policy, map of actions, MDP instance). 
 
%  \textbf{Finite-Horizon Policy}
% \begin{description}[1cm]
%     \item $Q$ 
%     \item $V$  
%     \item $policy$ 
%     \item $actions$ 
%     \item $include\:Q$ 
%     \item $MDP$ 
% \end{description}
 
 \textit{Mysolve} takes the Finite-Horizon MDP as input, initializes policy structure and value function matrix, and iterates from \textit{$(maximum\_horizon-1)^{th}$} to \textit{$1^{st}$} epoch. Each epoch is then evaluated using a value iteration algorithm, which is initialized with the following epoch's value function matrix. Results of each epoch are stored in \textit{FiniteHorizonPolicy}.
 
 \LinesNumbered
\begin{algorithm}
\SetAlgoLined
initialize $FiniteHorizonPolicy$ \\
initialize $V$ \\
\For{$epoch = horizon - 1;\ epoch > 0;\ epoch = epoch - 1$}{
    set \textit{epoch} global \\
    run 1 iteration of \textit{value\_iteration(MDP, epoch, V)} \\
    update \textit{FiniteHorizonPolicy} \\
}
return \textit{FiniteHorizonPolicy}
\caption{Finite-Horizon MDP Solver mysolve}
\end{algorithm}

In the current version of the algorithm, we have to set the epoch global to pass it to interface functions. The next planned version will no longer contain the global variable.

\subsection{Defining MDP}












POPSAT SPLNENI ZADANI FINITE HORIZON POMDPS




\section{solvers}

\subsection{approximate solvers}

\subsection{prunning}

jeste si nejsem jistej jestli ty dve predchozi kapitoly dam do implementace nebo do teorie

\section{PointBasedValueIteration.jl}
popsani toho jak to fungovalo predtim a jak jsem to upravil

\section{FiniteHorizonPointBasedValueIteration.jl}
popis implementace







Implementation

PBVI

The PBVI algorithm is stored in repo PointBasedValueIteration.jl and is partially based on former algorithm stored in the same place. The former algorithm, however, worker only for 2 states and completely omited the expansion phase, as the initial belief space was initiated with discrete distribution of beliefs.


The algorithm accepts problems defined in JuliaPOMDP/POMDPs.jl interface.



To some extent, the algorithm was written with list comprehensions or vectorizations, where it made sense. The utilization of vectorization was limited due to Julia's speed, often making the vectorizations slower than using for loops.



The PBVI's setting are handed over by the PBVISolver structure, which accepts following parameters:
num\_iteration, precision, verbose

The solve parameters are in line with POMDPs interface. That is, method solve accepts two parameters, solver's settings and POMDP to be solved. 


FIVI - TO BE COMPLETED 

















