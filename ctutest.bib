@article{cite:1,
    author = {BELLMAN, RICHARD},
    issn = {00959057, 19435274},
    journal = {Journal of Mathematics and Mechanics},
    number = {5},
    pages = {679--684},
    publisher = {Indiana University Mathematics Department},
    title = {{A Markovian Decision Process}},
    url = {http://www.jstor.org/stable/24900506},
    volume = {6},
    year = {1957}
}


@misc{ wiki:1,
    author = "{Wikipedia contributors}",
    title = "Markov decision process --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Markov_decision_process&oldid=995233484}",
    note = "[Online; accessed 7-January-2021]"
}

@article{JMLR:v18:16-300,
  author  = {Maxim Egorov and Zachary N. Sunberg and Edward Balaban and Tim A. Wheeler and Jayesh K. Gupta and Mykel J. Kochenderfer},
  title   = {POMDPs.jl: A Framework for Sequential Decision Making under Uncertainty},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {26},
  pages   = {1-5},
  url     = {http://jmlr.org/papers/v18/16-300.html}
}

@inbook{russel2010,
  added-at = {2020-02-01T18:23:11.000+0100},
  author = {Russell, Stuart and Norvig, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/20533b732950d1c5ab4ac12d4f32fe637/mialhoma},
  edition = 3,
  interhash = {53908a52dd4c6c8e39f93f4ffc8341be},
  intrahash = {0533b732950d1c5ab4ac12d4f32fe637},
  keywords = {ties4530},
  publisher = {Prentice Hall},
  timestamp = {2020-02-01T18:23:11.000+0100},
  title = {Artificial Intelligence: A Modern Approach},
  pages = {42-44},
  year = 2010
}

@article{Kolobov2012,
abstract = {Markov Decision Processes (MDPs) are widely popular in Artificial Intelligence for modeling sequential decision-making scenarios with probabilistic dynamics. They are the framework of choice when designing an intelligent agent that needs to act for long periods of time in an environment where its actions could have uncertain outcomes. MDPs are actively researched in two related subareas of AI, probabilistic planning and reinforcement learning. Probabilistic planning assumes known models for the agent's goals and domain dynamics, and focuses on determining how the agent should behave to achieve its objectives. On the other hand, reinforcement learning additionally learns these models based on the feedback the agent gets from the environment. This book provides a concise introduction to the use of MDPs for solving probabilistic planning problems, with an emphasis on the algorithmic perspective. It covers the whole spectrum of the field, from the basics to state-of-the-art optimal and approximation algorithms. We first describe the theoretical foundations of MDPs and the fundamental solution techniques for them. We then discuss modern optimal algorithms based on heuristic search and the use of structured representations. A major focus of the book is on the numerous approximation schemes for MDPs that have been developed in the AI literature. These include determinization-based approaches, sampling techniques, heuristic functions, dimensionality reduction, and hierarchical representations. Finally, we briefly introduce several extensions of the standard MDP classes that model and solve even more complex planning problems. Copyright {\textcopyright} 2012 by Morgan {\&} Claypool.},
author = {Kolobov, Mausam and Kolobov, Andrey},
doi = {10.2200/S00426ED1V01Y201206AIM017},
isbn = {9781608458868},
issn = {19394608},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
keywords = {AI planning,MDP,probabilistic planning,reinforcement learning,sequential decision making under uncertainty,uncertainty in AI},
pages = {1--203},
publisher = {Morgan {\&} Claypool publishers},
title = {{Planning with markov decision processes: An AI perspective}},
url = {https://drive.google.com/drive/u/0/folders/1HKSlbtEmkHtF4G1{\_}8TgPq7LYqCmFpmoH},
volume = {17},
year = {2012}
}


@misc{JuliaPOMDP,
  author = {Keith, Andrew and Egorov, Maxim and Peters, Lasse and Kochenderfer, Mykel},
  title = {JuliaPOMDP},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub Documentation},
  howpublished = {\url{https://juliapomdp.github.io/POMDPs.jl/latest/}},
  note = "[Online; accessed 10-January-2021]"
}

@misc{FHPOMDP,
  author = {Omasta, Tomas},
  title = {JuliaPOMDP/FiniteHorizonPOMDPs.jl},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/JuliaPOMDP/FiniteHorizonPOMDPs.jl}},
  note = "[Online; accessed 10-January-2021]"
}

@misc{JuliaLang,
  author = {JuliaLang},
  title = {JuliaLang/julia},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/JuliaPOMDP/FiniteHorizonPOMDPs.jl}},
  note = "[Online; accessed 10-January-2021]"
}

@misc{DVI,
  author = {Egorov, Maxim},
  title = {JuliaPOMDP/DiscreteValueIteration.jl},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/JuliaLang/julia}},
  note = "[Online; accessed 10-January-2021]"
}

@misc{JuliaStars,
   author = {Qian, Tim},
   year = {2015},
   title = {History of stars at JuliaLang\\julia},
   url = {\url{https://star-history.t9t.io/#JuliaLang\\julia}}
   }
   
@misc{JuliaHistory,
   author =       {Tung, Liam},
   title =        {Is Julia fastest-growing new programming language? Stats chart rapid rise in 2018},
   editor =       {zdnet.com},
   month =        {January},
   year =         {2019},
   url = {https://www.zdnet.com/article/is-julia-fastest-growing-new-programming-language-stats-chart-rapid-rise-in-2018/},
   note =         {[Online; posted 24-January-2019]},
 }

@misc{JuliaLangorg,
   author = {JuliaLang},
   title = {Julia home page},
   year = 2020,
   url = {https://julialang.org/},
   urldate = {2021-01-10}
}

@Article{Shani2013,
author={Shani, Guy
and Pineau, Joelle
and Kaplow, Robert},
title={A survey of point-based POMDP solvers},
journal={Autonomous Agents and Multi-Agent Systems},
year={2013},
month={Jul},
day={01},
volume={27},
number={1},
pages={1-51},
abstract={The past decade has seen a significant breakthrough in research on solving partially observable Markov decision processes (POMDPs). Where past solvers could not scale beyond perhaps a dozen states, modern solvers can handle complex domains with many thousands of states. This breakthrough was mainly due to the idea of restricting value function computations to a finite subset of the belief space, permitting only local value updates for this subset. This approach, known as point-based value iteration, avoids the exponential growth of the value function, and is thus applicable for domains with longer horizons, even with relatively large state spaces. Many extensions were suggested to this basic idea, focusing on various aspects of the algorithm---mainly the selection of the belief space subset, and the order of value function updates. In this survey, we walk the reader through the fundamentals of point-based value iteration, explaining the main concepts and ideas. Then, we survey the major extensions to the basic algorithm, discussing their merits. Finally, we include an extensive empirical analysis using well known benchmarks, in order to shed light on the strengths and limitations of the various approaches.},
issn={1573-7454},
doi={10.1007/s10458-012-9200-2},
url={https://doi.org/10.1007/s10458-012-9200-2}
}


@article{Walraven19,
author = {Walraven, Erwin and Spaan, Matthijs T. J.},
title = {Point-Based Value Iteration for Finite-Horizon POMDPs},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11324},
doi = {10.1613/jair.1.11324},
abstract = {Partially Observable Markov Decision Processes (POMDPs) are a popular formalism for sequential decision making in partially observable environments. Since solving POMDPs to optimality is a difficult task, point-based value iteration methods are widely used. These methods compute an approximate POMDP solution, and in some cases they even provide guarantees on the solution quality, but these algorithms have been designed for problems with an infinite planning horizon. In this paper we discuss why state-of-the-art point-based algorithms cannot be easily applied to finite-horizon problems that do not include discounting. Subsequently, we present a general point-based value iteration algorithm for finite-horizon problems which provides solutions with guarantees on solution quality. Furthermore, we introduce two heuristics to reduce the number of belief points considered during execution, which lowers the computational requirements. In experiments we demonstrate that the algorithm is an effective method for solving finite-horizon POMDPs.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {307â€“341},
numpages = {35}
}