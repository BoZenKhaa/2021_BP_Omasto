%!TEX ROOT=ctutest.tex


\part{MDPs}

\chapter{Theory}

In the previous chapter, we have briefly introduced MDPs. In this chapter, we will build on that intuition by reviewing a small part of the theory behind them, and we will also show possible approaches to their solving.

\section{Environment}
The environment that we introduced so far contains states, actions, transition probabilities, and costs for each action. However, we did not assign any rules to it! 
Without these rules, one can not create any connection between states and actions or actions and transition probabilities.

The environmental rules could range anywhere from the ones applied in the crossword puzzle game (fully observable, deterministic, sequential, static, discrete with single-agent) up to the ones applied in the taxi driving planning (partially observable, stochastic, sequential, dynamic, continuous with multiple agents). We will not describe the meaning of all specific rules in this work, as it is out of its scope. In case the reader is interested, we refer to read  \cite{russel2010}.

\newpage


For the sake of simplicity, let us assume the following properties of the given environment:
\begin{description}
  \item[$\bullet$ Fully observable] The agent has all information needed about the environment.
  \item[$\bullet$ Stochastic] The movement of the agent is not certain. The result of movement can be different from the desired one as a result of noise,
  \item[$\bullet$ Sequential] The current action is going to have an impact on future actions,
  \item[$\bullet$ Static] The environment can not change.,
  \item[$\bullet$ Discrete] The environment has a finite number of states and actions.
\end{description}



\section{MDP and its features}
With environmental properties sorted out, let us move on to MDPs themselves. 
\\ 
\\
The definitions, algorithms and the construction of following chapters draw on from \cite{Kolobov2012}. 
  \\ \\

\begin{definition}[\textbf{Finite Discrete-Time Fully Observable Markov Decision Process (MDP)}]\label{def:MDP}
A fully observable MDP is a tuple $(S, A, D, T, R)$, where:
\begin{description}
  \item[$\bullet$ ] $S$ is the finite set of all possible states of the system, also called the state space;
  \item[$\bullet$ ] $A$ is the finite set of all actions an agent can take;
  \item[$\bullet$ ] $D$ is a finite or infinite sequence of the natural numbers of the form $(1, 2, 3, \ldots, T_{max})$ or $(1, 2, 3, \ldots)$ respectively, 
        denoting the decision epochs, also called time steps, at which agent needs to act;
  \item[$\bullet$ ] $T : S \times A \times S \times D \rightarrow [ \,0, 1] \,$ is a transition function, a mapping specifying the probability $T(s_1, a, s_2, t)$ of going to state $s_2$ if action $a$ is executed when the agent is in state $s_1$ at time step $t$;
  \item[$\bullet$ ] $R : S \times A \times S \times D \rightarrow R$ is a reward function that gives a finite numeric reward value $R(s_1, a, s_2, t)$ obtained when the system goes from state $s_1$ to state $s_2$ as a result of executing action $a$ at time step $t$.
\end{description}

\end{definition}

Every MDP is solving its problem according to its movement policies. As the number of policies itself can be high order polynomial, we are going to relax their definition as well.
\\ \\
\begin{definition}[\textbf{Markovian Policy}]
A probabilistic (deterministic) history-dependent policy $\pi: H \times A \rightarrow [ \,0, 1] \,(\pi: H \rightarrow A)$ is \textit{Markovian} if for any two histories $h_{s,t}$ and $h'_{s,t}$, both of which end in the same state $s$ at the same time step $t$, and for any action $a$, 
$\pi(h_{s,t}, a) = \pi(h'_{s,t}, a)$ $(\pi(h_{s,t}) = a$ if and only if $\pi(h_{s,t}) = a)$.
\end{definition}

Every state in every time step is then evaluated according to \textbf{value function}. A Markovian value function then corresponds to $ V: S \times D 
\rightarrow [ \,-\infty, \infty] \,$. In the following text we will refer to value function as $V(s, t)$ or $V(s)$.

The value function of a policy is then: \\ \\

\begin{definition}[\textbf{The Value Function of a Policy}]
Let $h_{s,t}$ be a history that terminates at state $s$ and time $t$. Let $R^{\pi_{h_{s,t}}}_{t'}$ be random variables for the amount of reward obtained in an MDP as a result of executing policy $\pi$ starting in state $s$ for all time steps $t'$ s.t. $t \leqslant t' \leqslant |D|$ if the MDP ended up in state $s$ at time $t$ via history $h_{s,t}$.The value function $V^{\pi}: H \rightarrow [ \,−\infty,\infty] \,$ \textit{of a history-dependent policy} $\pi$ is a utility function $u$ of the reward sequence $R^{\pi_{h_{s,t}}}_t, R^{\pi_{h_{s,t}}}_{t+1}, \ldots$ that one can accumulate by executing $\pi$ at time steps $t, t + 1,\ldots$  after history $h_{s,t}$. Mathematically, $V^{\pi} (h_{s,t}) = u(R^{\pi_{h_{s,t}}}_t , R^{\pi_{h_{s,t}}}_{t+1}, \ldots)$.
\end{definition}

Among the policies evaluated according to the previous rule, we will be able to find an \textbf{optimal MDP solution} denoted as $\pi^*$ with value $V^*$ called the optimal value function. Such optimal solution then satisfies $V^{*} (h) \leqslant V^{\pi} (h)$ for all histories $h \in H$.

One of the possible approaches to evaluate the value function is using Expected Linear Additive Utility.
\\
\begin{definition}[\textbf{Expected Linear Additive Utility}]
An \textit{expected linear additive utility} function is a function $u(R_t, R_{t+1}, \ldots) = E[ \,\sum_{t'=t}^{|D|} \gamma ^{t'−t} R_{t'}] \, = E[ \,\sum_{t'=0}^{|D|-t} \gamma^{t'} R_{t'+t}] \,$ that computes the utility of a
reward sequence as the expected sum of (possibly discounted) rewards in this sequence, where $\gamma \geqslant 0$
is the discount factor.
\end{definition}

This approach eliminates the possibility of multiple different solution values resulting from multiple stochastic runs of the same policy. It employs the expected value and does not need to perform vectors equality comparison as its results are scalars.

It turns out that this approach is even better as it guarantees a fundamental property of MDPs called \textbf{The Optimality Principle}, according to which \textit{among the policies that are evaluated by the expected linear additive utility, there exists a policy that is optimal at every time step.}

\section{MDPs techniques}

This part will briefly introduce a few fundamental algorithms for MDPs solving: Brute-Force algorithm, Policy Iteration, Value Iteration, and show a possible solution for its disadvantages: Prioritizations. \\
After reading this section, the reader should know the possibilities of solving MDP and be ready for the following section, where we will discuss the finite horizon's advantages.

\subsection{Brute-Force Algorithm}
As its name suggests, the Brute-force algorithm is the most naive algorithm, similar to all classes of problem-solving. In evaluating the problem, the method evaluates all possible combinations and chooses the best one.
Moreover, computer scientists are not using it in the vast majority of cases because the algorithm needs to evaluate $|S|^{|A|}$ policies.
$S$ in the previous equation represents states and $A$ represents actions.

However, as the number of states, or actions, rises, such evaluation becomes enormous. Another problem comes from the fact that actions in the environment are not deterministic and often cyclic. Both of these problems results in a steep complexity rise.

That is why another approach comes in.

\subsection{An iterative approach to policy evaluation}

The evaluation of cyclic environments requires a suitable equation that captures all movements, transitions, and rewards.
That means that every state's value function should correspond to the sum of rewards for moving towards successor states. Moreover, the agent should maximize its reward by getting to the goal, so the successor state's value function multiplied by its probability should also appear in the equation. Let us state the formula as following:\\
\begin{equation}
\begin{aligned}
V^{\pi} (s) & = 0 && \text{(if $s \in G$)} \\
& = sum_{s' \in S} T(s, \pi (s), s') [ \,C(s, \pi(s), s') + V^{\pi} (s')] \, && \text{(otherwise)}
\end{aligned}
\end{equation}

This system of linear equations reduced the complexity of the previous approach. It \textit{can} be solved using Gaussian Elimination in $O(|S|^3)$. It is still not efficient enough, but we use its idea to solve it with \textbf{an iterative approach}.

The iterative approach does precisely what it says. 
It starts with rough estimations and continuously works its way up to the same solution as the linear system. Besides, this solution is optimal \cite{Kolobov2012}. As it stores the previous iteration, it is a part of dynamic programming. Every iteration of this algorithm runs in $O(|S|^2)$.

\LinesNumbered
\begin{algorithm}[H]
\SetAlgoLined
//Assumption: $\pi$ is proper (ends up in a goal state) \\
initialize $V^{\pi}_0$ arbitrarily for each state \\
$n \xleftarrow{} 0$ \\
\Repeat{$ max_{s\in S} residual_n (s) \leq \epsilon$}{
$n \xleftarrow{} n + 1$ \\
\ForEach{$ s \in S $}{
compute $V^{\pi}_n (s) \xleftarrow{} \sum_{s' \in S} T (s, \pi (s), s') [ \,C(s, \pi (s), s') + V^{\pi}_{n−1} (s') ] \,$ \\
compute $residual_n (s) \xleftarrow{} |V^{\pi}_n (s) − V^{\pi}_{n−1} (s)| $ \\
}
}
return $V^{\pi}_n$
\caption{Iterative Policy Evaluation}
\end{algorithm}

We know how to evaluate the environment's states given some policy, but we did not yet describe how to choose the best policy for a given evaluation. 

\subsection{Policy iteration}
Given that the result of iterative policy evaluation is optimal after an undefined number of iterations for a specific policy, we can further improve it by iterating policy evaluation and policy improvement.
For every iteration, we have \textit{almost} optimal policy evaluation for the previous policy. We can improve the policy by iterating over possible actions from all states and choosing that action if it has a lower cost than the previous one. The algorithm stops when the policy remains without a change at the end of the following iteration.

\LinesNumbered
\begin{algorithm}[H]
\SetAlgoLined
initialize $\pi_0$ to be an arbitrary proper (ending in the goal state) policy \\
$n \xleftarrow{} 0$ \\
\Repeat{$\pi_n == \pi_{n−1}$ }{
    $n \xleftarrow{} n + 1$ \\
    Policy Evaluation: compute $V^{\pi_{n−1}}$ \\
    Policy Improvement: \\
    \ForEach{state $s \in S$}{
        $ \pi_n (s) \xleftarrow{} \pi_{n−1} (s) $ \\
        $ \forall a \in A$ compute $Q^{(V^{\pi_{n−1}})} (s, a) $ \\
        $ V_n (s) \xleftarrow{} min_{a \in A} Q^{(V^{\pi_{n−1}})} (s, a) $ \\
        \uIf{$ Q^{(V^{\pi_{n−1}})} (s, \pi_{n−1} (s)) > V_n (s)$}{
            $ \pi_n (s) \xleftarrow{} argmin_{a \in A} Q^{(V^{\pi_{n−1}})} (s, a)$
        }
    \textbf{end}
    }
}
return $\pi_n$
\caption{Policy Iteration}
\end{algorithm}

Until now, whenever we talked about policy iteration, we mentioned that the initial policy has to be proper.
That is because if we do not meet the initial condition, the policy evaluation step will diverge.
Nevertheless, another algorithm can solve this drawback.


\subsection{Value iteration}
Value iteration focuses on improving value functions of states, instead of evaluating the policy. The algorithm creates policy at its very end as the action that minimizes the cost in given state. Thanks to this property, the initial policies are not used in the algorithm, and can not lead to divergence.

\textit{The algorithm is based on \textbf{Bellman equations}, which mathematically express the optimal solution of an MDP.}

\begin{equation}
\begin{aligned}
V^* (s) & = 0 && \text{(if $s \in Q$)} \\
& = min_{a \in A} Q^* (s, a) &&  (s \notin G) \\ 
Q^* (s, a) & = \sum_{s' \in S} T(s, a, s')[ \,C(s, a, s') + V^*(s')] \, \\
\end{aligned}
\end{equation}

The equation's interpretation is minimizing the optimal value function of a given state. It can have value 0 if the state is among goal states, or value minimizing the result of performing given action and then following optimal policy if the state is not.

In order to approximate $V^* (s)$, the algorithm uses the so-called \textbf{Bellman backup} to improve the current $V_n (s)$ with $V_{n-1} (s')$.


\begin{equation}
\begin{aligned}
V_n (s) \xleftarrow{} min_{a \in A} \sum_{s' \in S} T(s, a, s') [ \,C(s, a, s') + V_{n - 1} (s')] \,
\end{aligned}
\end{equation}

Value iteration is iteratively improving its value function estimations and is guaranteed to converge to the optimal solution \cite{Kolobov2012}. The stopping criterion can be specified as either max residual, maximal difference between consecutive state value function estimations, or as the number of iterations.

\LinesNumbered
\begin{algorithm}
\SetAlgoLined
initialize $V_0$ arbitrarily for each state \\
$n \xleftarrow{} 0$ \\ 
\Repeat{ $max_{s \in S} \: residual_n $ (s) < $ \epsilon $ }{
    $n \xleftarrow{} n + 1$ \\
    \ForEach{$ s \in S $ }{
        compute $V_n (s)$ using Bellman backup at $s$ \\ 
        compute $residual_n (s) = |V_n (s) - V_{n-1} (s)|$
        }
    }
return greedy policy: $\pi^{V_n} (s) = argmin_{a \in A} \sum_{s' \in S} \tau(s, a, s')[ \,C(s, a, s') + V_n (s')] \,$
\caption{Value Iteration}
\end{algorithm}


\subsection{Prioritization}
\textit{One of the most significant drawbacks of Value Iteration is that it requires full sweeps of the state space}. This drawback results in many unnecessary value function evaluations because its value remains the same as the value function updates are heading from goal states at first. It can take lots of iterations to evaluate all the successor state's value functions. Furthermore, due to cyclic moves, the speed of convergence among states can differ.

Both of these problems often result in flawed performing algorithm executions.

Logical improvement is to, instead of iterating whole state spaces, iterate over each state separately. This has two consequences:
\begin{enumerate}
    \item We have to develop or choose an algorithm that prioritizes states and 
    \item does not starve some of them (meaning every state gets updated accordingly).
\end{enumerate}

This also means that we can no longer use the number of iterations as a termination condition because we do not know which states' value function and how often, were updated. The only terminal condition remains maximum residual.

\cite{Kolobov2012} formalizes the intuition as follows:

\LinesNumbered
\begin{algorithm}
\SetAlgoLined
initialize $V$ \\
initialize priority queue $q$ \\
\Repeat{ termination }{
    select state $s' = q.pop()$ \\
    compute V(s') using a Bellman backup at s'\\
    \ForEach{predecessor s of s', i.e. $\{s|\exists a [\, T(s, a, s') > 0] \,\}$}{
        compute $priority(s)$\\
        $q.push(s, priority(s))$\\
        }
    }
return greedy policy $\pi^{V}$
\caption{Prioritized Value Iteration}
\end{algorithm}

We are going to introduce a few priority metrics whose features generally differ and may be even diverging under specific conditions. For details, head over to \cite{Kolobov2012}

\subsubsection{Prioritized Sweeping} 

\textit{Prioritized sweeping estimates the expected change in the value of a state if a backup were to be performed on it now.}

\begin{equation}priority_{PS} (s) \xleftarrow{} max \Big\{ priority_{PS} (s), max_{a \in A} \big\{ T(s, a, s') Res^{V}(s')\big\} \Big\} \end{equation}

\subsubsection{Improved Prioritized Sweeping}

Improved Prioritized Sweeping employs the idea that states with fast converging value functions are to be prioritized. The consequence is that the algorithm first iterates the states near the goal and moves to other states only after the change between state iterations becomes small.

\begin{equation}priority_{IPS} (s) \xleftarrow{} \frac{Res^{V} (s)}{V (s)} \end{equation}

\subsubsection{Focused Dynamic Programming}

Focused Dynamic Programming is a particular case of prioritization techniques used when the start state $s_0$ is known. In such cases, the start case's knowledge can be employed and added as a penalty factor.
\newpage
\begin{equation}priority_{FDP} \xleftarrow{} h(s) + V(s)\end{equation}


where:
\begin{description}[1cm]
  \item[h(s)] \textit{is lower bound on the expected cost of reaching s from $s_0$}, and
  \item[V(s)] is regular value function for given state s.
\end{description}

In the previous sections, we have very briefly introduced the notion of MDPs and their solving methods. First, we showed the Brute-Force algorithm, which naively evaluated all possible policies. Then we discussed the value and policy iterations that improved the performance but still did lots of useless evaluations. And finally, we presented a solution that intelligently iterates through prioritized states. 

\section{Finite-Horizon MDP}

With all the necessary theory layed out. Let us present the Finite-Horizon MDP.\\ \\

\begin{definition}[\textbf{Finite-Horizon MDP}]
A finite-horizon MDP is an MDP as described in ~\ref{def:MDP} with a finite number of time steps, i.e., with $|D| = T_{max} < \infty$.
\end{definition}


The Infinite-Horizon MDPs discussed so far are a slightly different class of problems from Finite-Horizon MDPs. However, the solution of Finite-Horizon is somewhat similar to the prioritized value iteration of Infinite-Horizon MDPs. 
Moreover, each Infinity-Horizon MDP can be transformed into Finite-Horizon one.

This is critical because the Finite-Horizon MDP has an acyclic state space, and \textbf{the acyclic MDPs can be solved optimally using only one backup if used optimal backup order.} \cite{Kolobov2012}

On the other hand, the transformation to Finite-Horizon MDP polynomially increases the state space.

As the finite horizon number is known, we can evaluate all states' value functions from maximum horizon time and work our way down to starting time. This way, we evaluate each state's successor's value functions, employing optimal backup order and finding the optimal policy on the first pass.

In conclusion, in cases where the Infinite-Horizon methods are not sufficient, the MDPs can be transformed into Finite-Horizon ones. However, while solving the MDP in one pass, this transformation also blows up the state space.

