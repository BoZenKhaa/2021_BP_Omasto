\chapter{Implementation}

This chapter focuses on the practical part of the thesis. In the beginning, we present the POMDPs.jl library we extended with this work and the assigned tasks. Then, we follow up with the analysis and description of the implemented methods. The following chapter presents the benchmark problems, and in the end, we present the validations and benchmarks.

This paper aims to survey and extend the methods implemented in the POMDPs.jl library, emphasizing the finite horizon ones. To add any other methods or interfaces missing to implement such methods and maintain interoperability with the rest of the library. Furthermore, to evaluate the performance of the implemented methods on benchmarks and design new test instances, if applicable.

% And, if applicable, to benchmark the methods against other methods implemented in the library.


% \section{POMDPs.jl library}

% Before we dive into the description of the accomplished tasks. It is necessary to introduce the POMDPs.jl library itself. The library is based on a set of interfaces gradually built on each other. Such foundations allow the community to create an environment in which the users can almost seamlessly change the functionality of their code. 


% The interface offers unified methods for defining problems (such as \textit{states} for representing all states of the problem), or methods for executing the algorithm selected (\textit{solve} method has the same parameters for all solvers). It also unifies the structures for storing results in the policies (\textit{AlphaVectorPolicy} for storing $\alpha$-vectors of POMDPs) or structures for passing the parameters to the solver. Given these methods are defined in interface, they can be used without changes no matter the problem or the solver.


\section{POMDPs.jl}

POMDPs.jl \cite{egorov2017pomdps} is an open-source framework for solving MDPs and POMDPs written in Julia. It builds on the interface of the same name. Unlike other frameworks for solving MDPs, the POMDPs.jl supports solving POMDPs and is not dependent on a specific external problem definition format. The POMDPs.jl presents a fast, easy to learn and prototype modular tool while being versatile for new users.


Its design supports the possibility to easily 1) define new problems, 2) create new solvers, or 3) run experiments. Through the unified interface, the goals of POMDPs.jl are to simplify the process of adapting to the code written by others, offer simple benchmarking, and mainly build an open-source environment open to new contributions.

The main advantages of POMDPs.jl are thus: 
\begin{enumerate}
    \item Its \textbf{simplicity} thanks to providing \textit{a minimal set of types and functions necessary to define a problem, a solver, and experiments in a partially observable setting}.
    \item Its \textbf{expressiveness} of interfaces providing \textit{the flexibility to solve both fully or partially observable problems, continuous or discrete}. Furthermore, this paper extends interfaces by allowing for differing between infinite and finite horizon POMDPs. It also supports using \textit{explicitly defined distributions or generative models. The combination of POMDPs.jl interfaces and Julia Languages makes it easy to prototype, effectively removing the need for problem definitions of another format.}
    \item Its interface \textbf{extensibility} by \textit{allowing new algorithms to be implemented with minimal effort.}
    \item Its \textbf{usability} by offering more than 20 solvers of various qualities that need at most four rows to run. 
\end{enumerate} 


The interface offers unified methods for defining problems (such as \textit{states} for representing all states of the problem), or methods for executing the selected algorithm (\textit{solve} method has the same parameters for all solvers). It also unifies the structures for storing resulting policies (\textit{AlphaVectorPolicy} for storing $\alpha$-vectors and corresponding actions of POMDP policies) or structures for passing the parameters to the solver. Given that the interface defines these methods, the users can use them without changes, no matter the problem or the solver.



% Bylo by hezke zakomponovat i nazvy jednotlivych dalsich modulu
% JuliaPOMDP's base method is the interface called POMDPs.jl. This interface defines all methods ranging from defining the model, through storing the result, to defining and executing solvers. On top of these methods, the user can either define a new problem or use already defined one in POMDPModels.jl repository. The user can also define a new solver, or use already defined one. Furthermore, the solvers return their results wrapped into a policy dedicated for it in POMDPPolicies.jl. If implementing a solver, the user would probably use the tools defined in POMDPModelTools.jl as, for example, an `ordered\_vector` which returns the wanted vector in an ordered manner, or distributions defined in the same repository.

% ZMINIT ZE JE TO DIKY TOU INTEROPERABLE

% The library also contains a lot of solvers for both MDPs and POMDPs. Ranging from Value Iteration up to Monte Carlo Tree Search solver for MDPs (4 total) and from approximate QMDP and other approximate solvers up to SARSOP or Incremental Prunning for POMDPs (12 total). Some of these solvers are reaching optimal solutions and some, usually approximate solvers, only suboptimal results, meaning that some are used more than the others. And they are also evaluated accordingly. However, with toal of 18 solvers (with 2 more reinforcement solvers) are offering more than enough to choose from when solving Infinite Horizon (PO)MDPs. 


\section{Assignment}
At the assignment time, the POMDPs.jl library does not include the Finite Horizon POMDP solvers, which are the main task of this work. The interface deemed essential for such solvers has been proposed but not completed. Such an interface is needed as the solvers need to operate over single stages while the current interface supports only handling the whole problem representation. Because of that, the objective of this thesis is to implement the interface for Finite Horizon POMDPs and to implement the selected algorithms as a proof of work. For that, we selected the Finite Horizon Value Iteration for MDPs and Point-Based Finite Horizon Value Iteration for POMDPs. These algorithms are the foundations for many improvements already published, making them an excellent starting point for future additions to the library. These algorithms are introduced in \cite{Walraven19} or \cite{Shani2013} and are recommended implementations to any reader interested in developing a new algorithm for the POMDPs.jl library. 

We were to evaluate the implemented algorithms against their infinite horizon counterparts. However, while implementing them, we found out that the author of the infinite horizon point-based value iteration POMDPs.jl algorithm, which we formerly wanted to use as a benchmark, implemented it to support only specific POMDPs with two states. Thus, we extended our work to fix and update the infinite horizon point-based value iteration.


\section{Finite Horizon POMDPs}

This chapter introduces the interface for defining Finite Horizon POMDPs. We implemented the interface in line with the initial design proposed by the POMDPs.jl co-author Zachary Sunberg in May 2019. Zachary Sunberg also wrote the minimal working example to preserve the interoperability and the idea of the library. The extensions and improvements are part of our work.


The goal of the Finite Horizon POMDPs interface was to create a POMDPs.jl-compatible interface for defining MDPs and POMDPs with finite horizons. 
Its key ideas were in particular, to:
\begin{itemize}
    \item provide a way for value-iteration-based algorithms to start at the final stage and work backwards,
    \item be compatible with generic POMDPs.jl solvers and simulators,
    \item provide a Finite-Horizon wrapper for Infinite-Horizon MDPs, and
    \item be compatible with other interface extensions like constrained POMDPs and mixed observability problems.
\end{itemize}

And its implementation was to contain the declaration of the following interface functions:

\begin{itemize}
    \item \textit{HorizonLength(::Type{<:Union{MDP,POMDP}}) = InfiniteHorizon()}
    \begin{itemize}
        \item \textit{FiniteHorizon}
        \item \textit{InfiniteHorizon}
    \end{itemize}
    \item \textit{horizon(m::Union{MDP,POMDP})::Int}
    \item \textit{stage\_states(m::Union{MDP,POMDP}, t::Int)}
    \item \textit{stage\_stateindex(m::Union{MDP,POMDP}, t::Int, s)}
    \item \textit{stage\_actions(m::Union{MDP,POMDP}, t::Int, [s])}
\end{itemize}

The whole idea of the interface lies on the concept of the \textit{HorizonLength} type, which returns the \textit{FiniteHorizon} or \textit{InfiniteHorizon} type corresponding to either a (PO)MDP being a finite or infinite horizon. In the case where (PO)MDP is a finite horizon, we can obtain the horizon value by calling the \textit{horizon} method and the current stage by calling \textit{stage} method. Furthermore, the interface extends the methods defined for infinite horizon problems by declaring their finite horizon counterparts. Methods belonging to this group are $stage\_states$, which returns states for given stage only, $stage\_stateindex$ which returns the index of a given state in a given stage; $ stage\_observations$ which returns observations for a given stage and $stage\_obsindex$, which returns the index of a given observation in a given stage. FiniteHorizonPOMDPs.jl also defines \textit{ordered\_stage\_states} and \textit{ordered\_stage\_observations} which return ordered states or observations of a given stage. At the time of writing this work, the \textit{stage\_actions} method is not implemented, as most of us known (PO)MDP problems contain actions that span all stages. This view is shared by the JuliaPOMDP community as well.

By using the combination of methods from FiniteHorizonPOMDPs.jl and other interfaces, the user can define his own finite horizon (PO)MDP problem, (PO)MDP solver or other (PO)MDP tool. At the same time, we recognize it can be difficult, or even lengthy to define one. For exactly these cases, the interface contains the utility, whose goal is to transform the user-defined infinite horizon (PO)MDP into a finite horizon one.  The design concept of the proposal was formerly to define a simple function that would transform a received infinite horizon (PO)MDP into a finite horizon (PO)MDP wrapper:

\begin{samepage}
\begin{itemize}
    \item \textit{fixhorizon(m::Union\{MDP,POMDP\}, T::Int) creates one of}
    \begin{itemize}
        \item \textit{FiniteHorizonMDP\{S, A\} <: MDP\{Tuple\{S,Int\}, A\}}
        \item \textit{FiniteHorizonPOMDP\{S, A, O\} <: POMDP\{Tuple\{S,Int\}, A, O\}}
    \end{itemize}
\end{itemize}
\end{samepage}

Finally, this utility became a set of methods that could easily create a package of its own. However, it is necessary to keep the utility with its original interface to be readily available. 

Other than \textit{fixhorizon} method, the utility defines new output types to recognize transformed (PO)MDPs. These types are called \textit{FixedHorizonMDPWrapper} and \textit{FixedHorizonPOMDPWrapper}. If there is no need to differentiate between them, the third type, \textit{FHWrapper}, is used. Thanks to this distinction, Julia Programming Language can dispatch the correct method on each call dynamically.

To distinguish between the states of the infinite horizon (PO)MDP and the finite horizon one, the \textit{fixhorizon} utility introduces a new structure. It wraps the pairs of states or observations and stages into tuples \textit{(state, stage)}, encoding the stage information without much performance loss.

Having defined the type of (PO)MDP, its finite horizon states, and observations, the utility also defines other interface methods required by various solvers. Namely, the POMDPs.jl interface methods ensure proper functionality no matter the finality of (PO)MDP with $transition$, $reward$, $isterminal$, $actions$, $discount$ etc. Methods defined specifically for finite horizon (PO)MDPs from POMDPs.jl interface, which generally return all states or observations for all stages. For example, $states$, $stateindex$, $observations$ or $obsindex$. Moreover, finite horizon (PO)MDPs interface methods implemented in this interface and described in the text above, such as $stage\_states$. Furthermore, the $fixhorizon$ utility implements other methods from $POMDPModelTools.jl$, which defines distributions for (PO)MDPS or methods for ordering states, actions, and observations. The $fix_horizon$ utility also implements the methods from $Random.jl$, offering the mean to compute probabilistic data about the distributions - mean, mode, support, or pdf.




REVIDOVAT


To define the Finite Horizon (PO)MDP instance, we have to define the methods declared in \textit{JuliaPOMDP/POMDPs.jl} \cite{egorov2017pomdps} or from another library's package. The Finite-Horizon MDP Solver further requires: \textit{stage\_states}, which returns states of a given epoch, \textit{stage\_actions}, which return actions of a given epoch, and \textit{stage\_stateindex}, which return indices of a given state for a given epoch. For example of properly defined MDP see directory \textit{/test/instances}.

TODO: ZMINIT KDE SE NACHAZEJI IMPLEMENTACE





\section{Finite Horizon Value Iteration}

\section{DOPSAT PODLE JAKEHO ALGORITMU Z TEORIE JE TOHLE NAPSANE, DOPLNIT REFERENCI}

The Finite Horizon Value Iteration extends the original Value Iteration algorithm. The algorithm iterates over the stages in reversed order from the last stage (excluding the terminal stage) to the first one and solves each stage with the Value Iteration algorithm.

Since this algorithm offers a one-pass optimal solution, after each iteration of solving Value Iteration, its result updates the resulting policy.

The algorithm is written in line with the POMDPs.jl solvers' architecture, consisting of three essential pillars - the structure for storing the algorithm parameters \textit{FiniteHorizonSolver}, the policy structure \textit{FiiteHorizoVakuePolicy} and the method executing the algorithm, \textit{solve}.

As the algorithm is one-pass, the \textit{FiniteHorizonSolver} structure contains only the boolean \textit{verbose} parameter, denoting whether to print debugging output or not.

\begin{minted}{julia}
struct FiniteHorizonSolver <: Solver 
    verbose::Bool 
end 
\end{minted}

The algorithm policy \textit{FiniteHorizonValuePolicy} contains the \textit{Q} matrix consisting of state and action index mapping to value, \textit{util} array mapping given state index to the best reward obtained after executing all actions executable from given state, \textit{policy} array mapping given state index to the action index corresponding to the utility value of given state. The \textit{include\_Q} boolean flag for storing the \textit{Q} matrix and \textit{m} for storing the MDP problem definition.


\begin{samepage}
\begin{minted}{julia}
mutable struct FiniteHorizonValuePolicy{Q<:Array, U<:Array, 
                P<:Array, A<:Array, M<:MDP} <: Policy
    qmat::Q
    util::U
    policy::P
    action_map::A
    include_Q::Bool
    m::M
end
\end{minted}
\end{samepage}

The solver also defines \textit{value} and \textit{action} methods used to give the value or the action of the given state.

The \textit{solve} method check whether the MDP is a Finite Horizon one, initializes the policy and utility vector, and then reverse iterates over stages (excluding the terminal stage), executing the Value Iteration and updating the policy in each iteration.

\begin{samepage}
\begin{minted}{julia}
function solve(solver::FiniteHorizonSolver, m::MDP)
    # check finality of MDP
    # initialize policy and utility 
    
    # iterate reversely over stages
        # execute Value Iteration over the given stage
        # update policy with the result of Value Iteration

    # return policy
end
\end{minted}
\end{samepage}


TODO: KONTROLA, PRIPADNE PREPSAT \\
The implementation is at \cite{FHPOMDP} or APENDIX.


\section{PointBasedValueIteration.jl}

The Point-Based Value Iteration (PBVI, JuliaPOMDP/PointBasedValueIteration.jl) algorithm is the second version of the package. However, the former algorithm worked only for two states and completely omitted the expansion phase. The former algorithm initiated the initial belief space with the uniformly distributed beliefs. We fully reimplemented the former algorithm but inspired ourselves with the former version.


The algorithm accepts problems defined in JuliaPOMDP/POMDPs.jl interface.

We hand over the PBVI's setting by the \textit{PBVISolver} structure, which accepts the following parameters: \textit{num\_iteration} denoting the maximal number of iterations the solver is to run, \textit{$\epsilon$} denoting the maximal gap between the $\alpha$-vectors from following iterations of step improvement and \textit{verbose} storing the boolean flag for printing the debug output.

\begin{samepage}
\begin{minted}[escapeinside=||,mathescape=true]{Julia}
struct PBVISolver <: Solver
    max_iterations::Int64
    |$\epsilon$|::Float64
    verbose::Bool
end
\end{minted}
\end{samepage}


The PBVI uses the \textit{AlphaVectorPolicy} defined in \textit{JuliaPOMDP/POMDPPolicies.jl}. The \textit{AlphaVectorPolicy} contains the POMDP \textit{pomdp} to which the policy belongs, \textit{n\_states} storing the number of states, a list of $\alpha$-vectors \textit{alphas}, and a mapping from $\alpha$-vectors to actions \textit{$action\_map$}.

\begin{samepage}
\begin{minted}{Julia}
struct AlphaVectorPolicy{P<:POMDP, A} <: Policy
    pomdp::P # needed for mapping states to locations in alpha vectors
    n_states::Int
    alphas::Vector{Vector{Float64}}
    action_map::Vector{A}
end
\end{minted}
\end{samepage}


The solve parameters are in line with the POMDPs interface. That is, method solve accepts two parameters, solver's settings and POMDP to be solved. The algorithm is the same as in [TODO: reference the algorithm in theory]. It first initializes all necessary representations and then iterates over the $\alpha$-vectors improvement and belief space expansion. If the expansion algorithm does not find any new belief worth visiting, it terminates the algorithm.


\begin{samepage}
\begin{minted}[escapeinside=||,mathescape=true]{Julia}
function solve(solver::PBVISolver, pomdp::POMDP)
    # initialize all POMDP representations

    # iterate up to $\textit{max\_iterations}$ times
        # improve $\alpha$-vectors
        # expand belief space
        # check whether the belief space expanded, if not terminate early
    
    # return $AlphaVectorPolicy$
end
\end{minted}
\end{samepage}


\section{Finite Horizon Point-Based Value Iteration}
Finite Horizon Point-Based Value Iteration (FiVI, TODO: PRIDAT ODKAZ NA REPO) is the state of the art algorithm that leverages the point-based approach and employs the heuristics of the best performing infinite horizon POMDP problem solvers. 

DOPSAT DALSI VECI






The solver accepts its parameters in form of \textit{FiVISolver} structure. \textit{FiVISolver} supports two arguments. \textit{precision} denoteing the maximum tolerance in the gap between upper and lower bound and \textit{time-limit}.

\begin{samepage}
\begin{minted}{Julia}
struct FiVISolver <: Solver
    precision::Float64
    time_limit::Int64
end
\end{minted}
\end{samepage}



The solver outputs the resulting policy and the upper bound of the initial belief. The policy is stored in the \textit{StagedPolicy} which we contributed with to the \textit{POMDPPolicies.jl}. The \textit{StagedPolicy} stores the array of the resulting policies for each stage \textit{staged\_policies} and the problem definition \textit{pomdp}. 


\begin{samepage}
\begin{minted}{Julia}
struct StagedPolicy{M<:FiniteHorizonPOMDPs,
                        FHWrapper, P<:Policy}<:Policy
    pomdp::M
    staged_policies::Array{P, 1}
end
\end{minted}
\end{samepage}



The \textit{solve} method definition is based on the interface, it accepts the algorithm parameters and the problem definition. The algorithm checks whether the problem is a finite horizon POMDP, initiates all variables needed to run the algorithm and sets the upper bound of the beliefs in the terminal stage to zero. The algorithm then iterates in the outer loop until one of terminal conditions evaluates to true. The inner loop iterates over the stages in the reversed order. It sequentially empties the vector of $alpha$-vectors, backups the $\alpha$-vectors of beliefs and update their upper bound. If the terminality conditions evaluate to false, the algorithm expands its belief space.

Unlike in the \cite{Walraven19}, our algorithm expands its belief space as the last thing it does in the outer loop. By using this approach, the algorithm successfully executes. If the algorithm executed the belief space expansion at the beginning of the loop, it would end up calculating with the infinity initiated upper bounds of beliefs. This approach is also used by the algorithm's author in the implementation of the algorithm at (TODO REF: https://github.com/AlgTUDelft/ConstrainedPlanningToolbox/blob/master/src/main/java/algorithms/pomdp/cgcp/FiniteVI.java)



\begin{samepage}
\begin{minted}[escapeinside=||,mathescape=true]{Julia}
function solve(solver::FiVISolver, pomdp::POMDP)
    # check finality of $\textit{pomdp}$, if $\textit{pomdp}$ is infinite throw error

    # initiate empty vectors for $\alpha$-vectors, belief space and belief set
    # initialize elapsed time to zero
    
    # set upper bound of belief in terminal stage to zero
    
    # iterate while all terminal conditions evaluate to true
        # iterate stages in reversed order
            # empty the vector for $\alpha$-vectors in t-th stage

            # backup all belief in t-th stage with the $\alpha$-vectors 
            #       from t+1-th stage
            # update upper bound of all belief vectors with the belief 
            #       space from t+1-th stage
            
        # check terminal conditions (time elapsed > $\textit{time\_limit}$ 
        #           or gap lower then maximum allowed)
        
        # expand belief space and update belief set
        
    # initialize $\textit{StagedPolicy}$
    # return policy and upper bound of initial belief
end
\end{minted}
\end{samepage}
